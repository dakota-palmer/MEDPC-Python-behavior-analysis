# -*- coding: utf-8 -*-
"""
Created on Mon Aug 16 12:01:47 2021

@author: Dakota
"""
#%% import dependencies
import pandas as pd
import glob
import os
import numpy as np

#%% About:

#this script will load your medpc data from spreadsheets generated by the mpctoexcel python package.
#it will also associate this data with 1) subject and 2) session metadata from spreadsheets manually created by you.
#data is imported and put into a pandas dataframe, df
#the dataframe is then "tidied" so that each column is one variable and each row is an observation (one observation=1 event timestamp)
#result is the dataframe dfTidy

#hierarchical data structure (subject, session, trial, event) is retained using labels for each event  

#finally, dfTidy is exported as .csv so it can be rapidly loaded later on without running this script.

# TODO: consider alternative organization like 'epoch' labels (e.g. DS, NS, in port, laser on, etc) 

#%% Things to manually change based on your data:
    
#datapath= path to your folder containing excel files

#colToImport= columns in your excel sheets to include (manually defined this so that I could exclude a specific variable that was huge)

#metapath= paths to 1) subject metadata spreadsheet (e.g. virus type, sex) and 2) session metadata spreadsheet (e.g. laser parameters, DREADD manipulations)
#excludeDate= specific date you might want to exclude

#eventVars= event type labels for recorded timestamps
#idVars=  subject & session metadata labels for recorded timestamps 


#%% ID and import raw data .xlsx
# your path to folder containing excel files 
datapath = r'C:\Users\Dakota\Desktop\Opto DS Task Test- Laser Manipulation\_dataRaw\\'


# set all .xls files in your folder to list
allfiles = glob.glob(datapath + "*.xls*")

#initialize list to store data from each file
dfRaw = pd.DataFrame()

#define columns in your .xlsx for specific variables you want (e.g. A:Z for all)
colToImport= 'F:S,U:X'

#for loop to aquire all excel files in folder
for excelfiles in allfiles:
    #read all sheets by specifying sheet_name = None
    #Remove any variables you don't want now before appending!
    #there was an issue with (W)trialState so leaving that out (col T)
    #Also leaving out first few columns
    raw_excel = pd.read_excel(excelfiles, sheet_name= None, usecols=colToImport)
    
    dfRaw = dfRaw.append(raw_excel, ignore_index=True)
    
#dfRaw is now nested df, each column is a subject and each row is a session

#eliminate data from 'MSNs' sheets for now, not informative currently
#TODO: this could be nice to get, but requires changing the mpc2excel scripts
dfRaw.drop('MSNs',axis=1,inplace=True)

#loop through nested df and append data. Now we have all data in one df
df= pd.DataFrame()

for subject in dfRaw.columns:
    print('loading'+subject)
    for file in range(len(dfRaw)):
        # print(allfiles[file]+subject)
        
        #add file label to each nested raw_excel before appending
        #assume fileName is yyyymmdd.xlsx (total of 13 characters at end of path. 5 are '.xlsx')
        dfRaw.loc[file,subject]['file']=allfiles[file][-13:]
        
        #add date too
        dfRaw.loc[file,subject]['date']= allfiles[file][-13:-5]
        
        
        #add subject label before appending
        dfRaw.loc[file,subject]['subject']=subject
        
        df= df.append(dfRaw.loc[file,subject])

#%% ID and import metadata .xlsx
#TODO: for now assuming separate excel files for these data

#convert subject and date variables to string datatype to ensure easy matching (excel number formatting can be weird)
df.subject= df.subject.astype('str')
df.date= df.date.astype('str')

# Match and insert subject metadata based on subject
metaPath= r"C:\Users\Dakota\Desktop\Opto DS Task Test- Laser Manipulation\_metadata\vp-vta-stgtacr_subj_metadata.xlsx"

dfRaw= pd.read_excel(metaPath).astype('str') 

df= df.merge(dfRaw.astype('str'), how='left', on=['subject'])

# Match and insert session metadata based on date and subject

metaPath= r"C:\Users\Dakota\Desktop\Opto DS Task Test- Laser Manipulation\_metadata\vp-vta-stgtacr_session_metadata.xlsx"

dfRaw= pd.read_excel(metaPath).astype('str') 

df= df.merge(dfRaw.astype('str'), how='left', on=['subject','date'])


# %% Exclude data

excludeDate= ['20210604']

# Exclude specific date
df = df[df.date != excludeDate]

# %% Remove parentheses from variable names 

import re
#use regex to replace text between () with empty string
#loop through each column name, remove characters between () and collect into list 'labels'
labels= []
for col in df.columns:
    labels.append(re.sub(r" ?\([^)]+\)", "", col))
#rename columns to labels
df.columns= labels

# %% Add other variables if necessary before tidying

# calculate port exit time estimate using PEtime and peDur, save this as a new variable
df = df.assign(PExEst=df.PEtime + df.PEdur)



#%% Add unique fileID for each session (subject & date )

df.loc[:,'fileID'] = df.groupby(['date', 'subject']).ngroup()

#%% Define Event variables for your experiment 
#make a list of all of the Event Types so that we can melt them together into one variable
#instead of one column for each event's timestamps, will get one single column for timestamps and another single column for the eventType label

#these should match the labels in your .MPC file

#e.g. for DS task with Opto
eventVars= ['PEtime', 'PExEst', 'lickTime', 'laserTime', 'DStime', 'NStime', 'UStime','laserOffTime']

## e.g. for DS task with no Opto  
# eventVars= ['PEtime', 'PExEst', 'lickTime', 'DStime', 'NStime', 'UStime']

#%% Define ID variables for your sessions
#these are identifying variables per sessions that should be matched up with the corresponding event variables and timestamps
#they should variables in your session and subject metadata spreadsheets

#e.g. for DS task with Opto
idVars= ['fileID','subject', 'RatID', 'Virus', 'Sex', 'date', 'laserDur', 'note']

## e.g. for DS task with no Opto
# idVars= ['fileID','subject', 'RatID', 'Virus', 'Sex', 'date', 'note']

#%% Define Trial variables for your experiment
# If you have variables corresponding to each individual trial 
#e.g. different trial types in addition to DS vs NS (e.g. laser ON vs laser OFF trials; TODO: variable reward outcome)

#e.g. for Opto:
trialVarsLaser= ['laserDStrial','laserNStrial']
#the laserDStrial and laserNS trial variables will later be melted() into a new variable called 'laserState' with their values

#%% Tidy df org: All events in single column, sort by time by file, with fileID column and trialID column that matches trial 1-60 through each session.

#First, am melting columns of behavioral events into single column of event label and column of individual timestamps (value_vars= want to melt)
dfEventAll = df.melt(id_vars=idVars, value_vars=eventVars, var_name='eventType', value_name='eventTime') #, ignore_index=False)

#Remove all rows with NaN eventTimes (these are just placeholders, not valid observations) 
dfEventAll= dfEventAll[dfEventAll.eventTime.notna()]

# remove invalid/placeholder 0s
# TODO: seem to be removing legitimate port exits with peDur==0, not sure how to deal with this so just excluding
dfEventAll = dfEventAll[dfEventAll.eventTime != 0]

dfEventAll.index.name= 'eventID'

# sort all event times by file and timestamp
# set index name so we can sort by it easily
# dfEventAll.index.name= 'fileID'
dfEventAll = dfEventAll.sort_values(by=['fileID', 'eventTime'])

 # need to reset_index before assigning values back to dfEventAll (this is because the fileID index repeats so assignment is ambiguous)
dfEventAll.reset_index(drop=True,inplace=False) #dropping old unsorted eventID
dfEventAll.index.name= 'eventID'



 # add trialID column by cumulative counting each DS or NS within each file
 # now we have ID for trials 0-59 matching DS or NS within each session, nan for other events
dfEventAll['trialID'] = dfEventAll[(dfEventAll.eventType == 'DStime') | (
    dfEventAll.eventType == 'NStime')].groupby('fileID').cumcount()

 #add trialType label using eventType (which will be DS or NS for valid trialIDs)
dfEventAll['trialType']= dfEventAll[dfEventAll.trialID.notna()].eventType

 #%% 
#sort again by file & event timestamp before saving to new df
dfEventAll = dfEventAll.sort_values(by=['fileID', 'eventTime'])
 
# #reset index so that eventIDs are in chronological order
# #for some reason inplace=true isn't working here so just reassign
# dfEventAll= dfEventAll.reset_index(drop=True)
# dfEventAll.index.name= 'eventID'


dfTidy = dfEventAll.copy()


#%% Assign trialTypes (OPTO ONLY specific for now)

#goal in this section will be to assign trialType (using trialVars) to each trial using trialIDs within session

#match up using unsorted eventIDs
#should melt to individual events retaining all columns and then melt again for trialVars

#want to keep all other columns as id_vars
# allIDvars= df.drop(eventVars,axis=1).columns
allIDvars= idVars+trialVars

#First, am melting columns of behavioral events into single column of event label and column of individual timestamps (value_vars= want to melt)
# dfEventAll = df.melt(id_vars=allIDvars, value_vars=eventVars, var_name='eventType', value_name='eventTime',ignore_index=False) 
dfEventAll = df.melt(id_vars=idVars, value_vars=eventVars, var_name='eventType', value_name='eventTime',ignore_index=False) 


#save index as new column with unique eventID
dfEventAll.index.name= 'eventID'
dfEventAll.reset_index(inplace=True)

#Remove all rows with NaN eventTimes (these are just placeholders, not valid observations) 
dfEventAll= dfEventAll[dfEventAll.eventTime.notna()]

# remove invalid/placeholder 0s
# TODO: seem to be removing legitimate port exits with peDur==0, not sure how to deal with this so just excluding
dfEventAll = dfEventAll[dfEventAll.eventTime != 0]

# #trialID= cumcount of trials within mpc file
dfEventAll['trialID'] = dfEventAll[(dfEventAll.eventType == 'DStime') | (
dfEventAll.eventType == 'NStime')].groupby('fileID').cumcount()  # .index

# before any removal/sorting is done, next issue is to match up laser state (trialVars) with cues.
# dfTrial= dfEventAll[(dfEventAll.eventType == 'DStime') | (dfEventAll.eventType == 'NStime')].copy()
dfTrial = df.melt(id_vars= idVars, value_vars=trialVars, var_name='laserType', value_name='laserState')#, ignore_index=False)
#remove nan placeholders
dfTrial= dfTrial[dfTrial.laserState.notna()]

#get trialID
dfTrial['trialID'] = dfTrial.groupby('fileID').cumcount() 

#merge trialType data back into df on matching fileID & trialID
dfEventAll= dfEventAll.merge(dfTrial[['trialID', 'fileID', 'laserType', 'laserState']], on=[
    'fileID', 'trialID'], how='left')#.drop('trialID', axis=1)

#combine laserState and laserType into one variable for labelling each trial: trialType
#Exclude the Lick-paired laser sessions. We will label those using a different method below  
dfEventAll.loc[dfEventAll.laserDur!='Lick', 'trialType'] = dfEventAll.laserType + \
    '_'+dfEventAll.laserState.astype(str).copy()

 
#sort again by file & event timestamp before saving to new df
dfEventAll = dfEventAll.sort_values(by=['fileID', 'eventTime'])


#%% Reassign eventIDs in chronological order
#reset index so that eventIDs are in chronological order
#for some reason inplace=true isn't working here so just reassign
dfEventAll= dfEventAll.reset_index(drop=True)
dfEventAll.index.name= 'eventID'

# %% Tidy df org: All events in single column, sort by time by file, with fileID column and trialID column that matches trial 1-60 through each session.
#also want a trialType column           
      
#FOR OPTO
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  
#First, am melting columns of behavioral events into single column of event label and column of individual timestamps (value_vars= want to melt)
dfEventAll = df.melt(id_vars=idVars, value_vars=eventVars, var_name='eventType', value_name='eventTime', ignore_index=False)
dfEventAll = df.melt(value_vars=eventVars, var_name='eventType', value_name='eventTime', ignore_index=False)


# remove invalid/placeholder 0s
# TODO: seem to be removing legitimate port exits with peDur==0, not sure how to deal with this so just excluding
dfEventAll = dfEventAll[dfEventAll.eventTime != 0]


# before any removal/sorting is done, next issue is to match up laser state (trialVars) with cues.
dfTrial= dfEventAll[(dfEventAll.eventType == 'DStime') | (dfEventAll.eventType == 'NStime')].copy()
dfTrial = dfTrial.melt(id_vars= idVars, value_vars=trialVars, var_name='laserType', value_name='laserState')#, ignore_index=False)

# dfTrial = df.melt(id_vars= idVars, value_vars=trialVars, var_name='laserType', value_name='laserState')#, ignore_index=False)


# use cumcount() of DS & NS to match up laser status with cue time (redundant since we have a timestamp of laser on, but good verification still and easy grouping by trial)
# dfEventAll.index.name = 'fileID'
# dfEventAll.reset_index(inplace=True)
dfEventAll.index.name= 'eventID'
#cumulative count of each trial start (cue onset) within files 
dfEventAll['trialCount'] = dfEventAll[(dfEventAll.eventType == 'DStime') | (
dfEventAll.eventType == 'NStime')].groupby('fileID').cumcount()  # .index


# dfTrial.index.name = 'fileID'
# dfTrial.reset_index(inplace=True)
dfTrial['trialCount'] = dfTrial[(dfTrial.laserType == 'laserDStrial') | (
dfTrial.laserType == 'laserNStrial')].groupby('fileID').cumcount()

print(dfTrial['trialCount'].shape, dfTrial['trialCount'].max(), dfEventAll['trialCount'].shape, dfEventAll['trialCount'].max())

dfEventAll = pd.merge(dfEventAll, dfTrial[['trialCount', 'fileID', 'laserType', 'laserState']], on=[
    'fileID', 'trialCount'], how='left').drop('trialCount', axis=1)

# sort all event times by file and timestamp
# set index name so we can sort by it easily
# dfEventAll.index.name= 'fileID'
dfEventAll = dfEventAll.sort_values(by=['fileID', 'eventTime'])

  # need to reset_index before assigning values back to dfEventAll (this is because the fileID index repeats so assignment is ambiguous)
dfEventAll.reset_index(drop=True,inplace=False) #dropping old unsorted eventID
dfEventAll.index.name= 'eventID'


  # add trialID column by cumulative counting each DS or NS within each file
  # now we have ID for trials 0-59 matching DS or NS within each session, nan for other events
dfEventAll['trialID'] = dfEventAll[(dfEventAll.eventType == 'DStime') | (
    dfEventAll.eventType == 'NStime')].groupby('fileID').cumcount()

# combine laserState and laserType into one variable for labelling each trial: trialType
#Exclude the Lick-paired laser sessions. We will label those using a different method below  
dfEventAll.loc[dfEventAll.laserDur!='Lick', 'trialType'] = dfEventAll.laserType + \
    '_'+dfEventAll.laserState.astype(str).copy()

 
#sort again by file & event timestamp before saving to new df
dfEventAll = dfEventAll.sort_values(by=['fileID', 'eventTime'])
 
#reset index so that eventIDs are in chronological order
#for some reason inplace=true isn't working here so just reassign
dfEventAll= dfEventAll.reset_index(drop=True)
dfEventAll.index.name= 'eventID'


dfTidy = dfEventAll.copy()


#%% Save dfTidy as .csv so it can be loaded quickly for subesequent analysis

# dfTidy.to_csv('dfTidy.csv')





#%% Custom method of groupby subsetting, manipulations, and reassignment to df
#TODO: in progress...
#May be interchangable with groupby.transform() Call function producing a like-indexed DataFrame on each group and return a DataFrame having the same indexes as the original object filled with the transformed values
def groupbyCustom(df, grouper):
    #df= dataframe ; grouper= list of columns to groupby (e.g.) grouper= ['subject','date'] 
    
    grouped= df.groupby(grouper)
    
    #get the unique groups
    groups= grouped.groups
    
        
    #Each group in groups contains index of items belonging to said group
    #so we can loop through each group, use that as a key for the groups dict to get index
    #then retrieve or alter values as needed by group with this index using df.iloc  
    # dfGroup= pd.DataFrame()
    #initialize dfGroup as all nan copy of original df. Then we'll get values by group
    dfGroup= df.copy()
    dfGroup[:]= np.nan
    
    #collection using loop takes too long. would be nice to vectorize (need to find a way to use the dict int64 ind as index in df I think)
    for group in groups:
        #index corresponding to this group in the df
        groupInd= groups[group]
        
        #extract values from df
        dfGroup.loc[groupInd,:]= df.loc[groupInd,:]
        
        #add label for this group
        # dfGroup.loc[groupInd,'groupID']= [group]
        dfGroup.loc[groupInd,'groupID']= str(group)
        
    # for group in groups:
    #     #use key and get_group
    #     #this approach isolates values but loses original index
    #     dfGroup= grouped.get_group(group)

        #here you could run a function on dfGroup
        return dfGroup